{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b03e4d9",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a0c206",
   "metadata": {},
   "source": [
    "## Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e6c2e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import time\n",
    "from pathlib import Path\n",
    "import re\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.losses import Huber\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "\n",
    "from itables import init_notebook_mode, show\n",
    "\n",
    "# Initialize itables for interactive tables in Jupyter Notebook\n",
    "init_notebook_mode(all_interactive=True)  # Initialize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0265d",
   "metadata": {},
   "source": [
    "## Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e44c06c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOL_ADDRESS = \"So11111111111111111111111111111111111111112\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "112dc614",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_view = [\"pairAddress\", \"baseToken.address\", \"averageLiquidity\", \"cfv_future\", \"cumulative_fee_volume\", \"percentageProfit\", \"pairAge\", \"baseToken.name\"]\n",
    "columns_to_describe = [\"cfv_future\", \"cumulative_fee_volume\", \"percentageProfit\"]\n",
    "\n",
    "# columns to keep\n",
    "numeric_fields = [\n",
    "    \"fees24h.m5\",           # Float64\n",
    "    \"volume.m5\",            # Float64\n",
    "    \"liquidity.meteora\",    # Float64\n",
    "    \"volume24h.h24\",        # Float64\n",
    "    \"volume.h1\",            # Float64\n",
    "    \"apr\",                  # Float64\n",
    "    \"txns.m5.sells\",        # Int64\n",
    "    \"priceChange.h1\",       # Float64\n",
    "    \"priceChange.m5\",       # Float64\n",
    "    \"txns.h1.buys\",         # Int64\n",
    "    \"txns.h6.sells\",        # Int64\n",
    "    \"txns.h1.sells\",        # Int64\n",
    "    \"reserve_x_amount\",     # Float64\n",
    "    \"volume24h.h1\",         # Float64\n",
    "    \"pairCreatedAt\",        # Int64\n",
    "    \"farm_apy\",             # Int64\n",
    "    \"feeToTvl.max\",         # Float64\n",
    "    \"fees_24h\",             # Float64\n",
    "    \"feeToTvl.h24\",         # Float64\n",
    "    \"txns.h24.sells\",       # Int64\n",
    "    \"fees24h.max\",          # Float64\n",
    "    \"liquidity.usd\",        # Float64\n",
    "    \"fdv\",                  # Int64\n",
    "    \"feeToTvl.h6\",          # Float64\n",
    "    \"txns.m5.buys\",         # Int64\n",
    "    \"cumulative_fee_volume\",# Float64\n",
    "    \"marketCap\",            # Int64\n",
    "    \"fees24h.h1\",           # Float64\n",
    "    \"liquidity.quote\",      # Float64\n",
    "    \"fees24h.h6\",           # Float64\n",
    "    \"volume24h.h6\",         # Float64\n",
    "    \"fees24h.h24\",          # Float64\n",
    "    \"volume24h.min\",        # Float64\n",
    "    \"volume.h24\",           # Float64\n",
    "    \"txns.h6.buys\",         # Int64\n",
    "    \"volume.h6\",            # Float64\n",
    "    \"base_fee\",             # Float64\n",
    "    \"feeToTvl.h1\",          # Float64\n",
    "    \"farm_apr\",             # Int64\n",
    "    \"priceChange.h6\",       # Float64\n",
    "    \"txns.h24.buys\",        # Int64\n",
    "    \"apy\",                  # Float64\n",
    "    \"volume24h.max\",        # Float64\n",
    "    \"feeToTvl.m5\",          # Float64\n",
    "    \"bin_step\",             # Int64\n",
    "    \"feeToTvl.min\",         # Float64\n",
    "    \"fees24h.min\",          # Float64\n",
    "    \"liquidity.base\",       # Float64\n",
    "    \"trade_volume_24h\",     # Float64\n",
    "    \"priceChange.h24\",      # Float64\n",
    "    \"today_fees\",           # Float64\n",
    "    \"volume24h.m5\",         # Float64\n",
    "    \"pairAge\",              # Int64\n",
    "    \"percentageProfit\"      # Float64\n",
    "]\n",
    "\n",
    "string_fields = [\n",
    "    \"cumulative_trade_volume\",  # String\n",
    "    \"priceUsd\",                 # String\n",
    "    \"liquidity\",                # String\n",
    "    \"base_fee_percentage\",      # String\n",
    "    \"protocol_fee_percentage\",  # String\n",
    "    \"priceNative\",              # String\n",
    "    # \"trend\",                    # String\n",
    "    \"max_fee_percentage\",       # String\n",
    "    \"reserve_y_amount\",         # String\n",
    "]\n",
    "\n",
    "bool_fields = [\n",
    "    # \"strict\",                   # Boolean\n",
    "    # \"hide\",                     # Boolean\n",
    "    \"isPumpToken\"               # Boolean\n",
    "]\n",
    "\n",
    "columns_to_keep = string_fields + numeric_fields + bool_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63959b",
   "metadata": {},
   "source": [
    "## Preprocessing util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27105031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_nested_values(json_data):\n",
    "    \"\"\"\n",
    "    Explodes nested fields in JSON data, flattening nested dictionaries and lists.\n",
    "    :param json_data: Parsed JSON data (list of dictionaries).\n",
    "    :return: Flattened JSON data.\n",
    "    \"\"\"\n",
    "    def flatten(record, parent_key='', sep='.'):\n",
    "        \"\"\"\n",
    "        Flattens a single dictionary by recursively expanding nested dictionaries and lists.\n",
    "        :param record: The dictionary to flatten.\n",
    "        :param parent_key: Base key for recursion.\n",
    "        :param sep: Separator for nested keys.\n",
    "        :return: A flattened dictionary.\n",
    "        \"\"\"\n",
    "        items = []\n",
    "        for key, value in record.items():\n",
    "            new_key = f\"{parent_key}{sep}{key}\" if parent_key else key\n",
    "\n",
    "            # temporary measure to avoid certain nested fields\n",
    "            keys_to_skip = set([\"boosts\", \"info\"])\n",
    "            if key in keys_to_skip:\n",
    "                continue\n",
    "            \n",
    "            if isinstance(value, dict):\n",
    "                items.extend(flatten(value, new_key, sep=sep).items())\n",
    "            elif isinstance(value, list):\n",
    "                # If the value is a list, index each element with its position\n",
    "                for i, item in enumerate(value):\n",
    "                    items.extend(flatten({f\"{new_key}[{i}]\": item}, sep=sep).items())\n",
    "            else:\n",
    "                items.append((new_key, value))\n",
    "        return dict(items)\n",
    "\n",
    "    return [flatten(record) for record in json_data]\n",
    "\n",
    "def find_and_update_large_int_fields(json_data):\n",
    "    \"\"\"\n",
    "    Find fields with large integers and update the JSON data to convert them to floats.\n",
    "    \"\"\"\n",
    "    max_supported_int = 9_223_372_036_854_775_807  # Maximum value for 64-bit integers\n",
    "    large_int_fields = set()\n",
    "\n",
    "    # Find fields with large integers\n",
    "    for record in json_data:\n",
    "        for key, value in record.items():\n",
    "            if isinstance(value, int) and value > max_supported_int:\n",
    "                large_int_fields.add(key)\n",
    "\n",
    "    # Update the JSON data: convert values in those fields to strings\n",
    "    for record in json_data:\n",
    "        for key in large_int_fields:\n",
    "            if key in record and isinstance(record[key], int):\n",
    "                record[key] = float(record[key])\n",
    "\n",
    "    return json_data, list(large_int_fields)\n",
    "\n",
    "def find_mixed_type_fields(json_data):\n",
    "    \"\"\"\n",
    "    Identify fields with mixed data types in the JSON data.\n",
    "    Store types and example values for manual inspection.\n",
    "    :param json_data: Parsed JSON data (list of dictionaries).\n",
    "    :return: A dictionary of fields with their types and example values.\n",
    "    \"\"\"\n",
    "    type_map = defaultdict(set)  # Store unique data types for each field\n",
    "    value_map = defaultdict(list)  # Store example values for each field\n",
    "\n",
    "    # Analyze each record\n",
    "    for record in json_data:\n",
    "        for key, value in record.items():\n",
    "            value_type = type(value).__name__\n",
    "            \n",
    "            if value_type not in type_map[key] or len(value_map[key]) < 5:  # Limit stored examples for readability\n",
    "                type_map[key].add(value_type)\n",
    "                value_map[key].append((value_type, value))\n",
    "\n",
    "    # Find fields with mixed types\n",
    "    mixed_type_fields = {\n",
    "        key: {\"types\": list(types), \"examples\": value_map[key]}  # Convert set to list\n",
    "        for key, types in type_map.items()\n",
    "        if len(types) > 1\n",
    "    }\n",
    "    return mixed_type_fields\n",
    "\n",
    "def normalize_mixed_types(json_data, mixed_type_fields):\n",
    "    \"\"\"\n",
    "    Normalize mixed types in the JSON data. Currently, we convert every\n",
    "    data type that has a float in its type to floats.\n",
    "    \"\"\"\n",
    "    field_types = defaultdict(set)\n",
    "    for field in mixed_type_fields:\n",
    "        field_types[field] = mixed_type_fields[field][\"types\"]\n",
    "\n",
    "    for record in json_data:\n",
    "        for field in mixed_type_fields:\n",
    "            if field in record:\n",
    "                value = record[field] if record[field] is not None else float(0)\n",
    "                if not isinstance(value, float) and \"float\" in field_types[field]:\n",
    "                    record[field] = float(value)  # Convert integers to floats for consistency\n",
    "                elif isinstance(value, float):\n",
    "                    pass\n",
    "                else:\n",
    "                    raise ValueError(f\"Did not handle field {field} with value {value} and types {field_types[field]}\")\n",
    "    return json_data\n",
    "\n",
    "def miscellaneous_cleaning(json_data):\n",
    "    for record in json_data:\n",
    "        force_conversions = [[float, [\"reserve_x_amount\", \"cumulative_fee_volume\"]]]\n",
    "        for conversion, fields in force_conversions:\n",
    "            for field in fields:\n",
    "                if field in record:\n",
    "                    try:\n",
    "                        record[field] = conversion(record[field])\n",
    "                    except ValueError:\n",
    "                        print(f\"Invalid {conversion} conversion: {record[field]}\")\n",
    "            \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae03918c",
   "metadata": {},
   "source": [
    "## Main loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfe2cec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_and_process(file_path, batch_size=1000):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            file_str = file.read()\n",
    "            json_data = json.loads(file_str)\n",
    "\n",
    "        \n",
    "        # Explode nested fields\n",
    "        json_data = explode_nested_values(json_data)\n",
    "\n",
    "        # Find and update fields with large integers\n",
    "        json_data, _ = find_and_update_large_int_fields(json_data)\n",
    "\n",
    "        # Find fields with mixed data types\n",
    "        mixed_type_fields = find_mixed_type_fields(json_data)\n",
    "        if mixed_type_fields and \"timestamp\" in mixed_type_fields:\n",
    "            print(\"mixed_type_fields: \", mixed_type_fields[\"timestamp\"])\n",
    "        if mixed_type_fields:\n",
    "            json_data = normalize_mixed_types(json_data, mixed_type_fields)\n",
    "\n",
    "        json_data = miscellaneous_cleaning(json_data)\n",
    "\n",
    "        # Process the JSON data in batches due to weird error \n",
    "        # that occurs when loading all data into a single dataframe\n",
    "        # in polars\n",
    "        all_batches = []\n",
    "        for i in range(0, len(json_data), batch_size):\n",
    "            batch = json_data[i:i + batch_size]\n",
    "            batch_df = pl.DataFrame(batch)\n",
    "            all_batches.append(batch_df)\n",
    "\n",
    "        # Concatenate all batches into a single Polars DataFrame\n",
    "        df = pl.concat(all_batches, how=\"vertical\")\n",
    "        return df, json_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_file(file, batch_size=100_000):\n",
    "    \"\"\"\n",
    "    Process a single JSON file: load, parse, and return metadata.\n",
    "    :param file: Path to the file to be processed.\n",
    "    :param batch_size: Batch size for processing large JSON data.\n",
    "    :return: Tuple (key_suffix, timestamp, subdir, DataFrame).\n",
    "    \"\"\"\n",
    "    file_pattern = re.compile(r\"(\\d+)_historical_data_(.+)\\.json\")\n",
    "    match = file_pattern.match(file.name)\n",
    "\n",
    "    if not match:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        timestamp = int(match.group(1))  # Extract timestamp\n",
    "        key_suffix = match.group(2)  # Extract key suffix\n",
    "\n",
    "        # Read JSON into a Polars DataFrame\n",
    "        df, json_data = read_file_and_process(file, batch_size=batch_size)\n",
    "\n",
    "        # Return processed data\n",
    "        return key_suffix, timestamp, file.parent.name, df, json_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_and_group_json_data_by_suffix_parallel(base_dir, max_files=None):\n",
    "    \"\"\"\n",
    "    Load and group JSON data by key suffix using parallel processing.\n",
    "    :param base_dir: Base directory containing subdirectories with JSON files.\n",
    "    :return: Sorted dictionary of grouped data.\n",
    "    \"\"\"\n",
    "    # Directories to process\n",
    "    subdirs = [\"dex_screener_pairs\", \"enriched_data\", \"meteora_pairs\"]\n",
    "    grouped_data = {}\n",
    "\n",
    "    # Find all JSON files to process\n",
    "    files_to_process = []\n",
    "    # file_pattern = re.compile(r\"(\\d+)_historical_data_(.+)\\.json\")\n",
    "\n",
    "    for subdir in subdirs:\n",
    "        full_path = Path(base_dir) / subdir\n",
    "        if not full_path.is_dir():\n",
    "            print(f\"Skipping non-existent directory: {full_path}\")\n",
    "            continue\n",
    "        if max_files:\n",
    "            files_to_process.extend(list(full_path.glob(\"*.json\"))[:max_files])\n",
    "        else:\n",
    "            files_to_process.extend(full_path.glob(\"*.json\"))\n",
    "\n",
    "    # Process files in parallel with a progress bar\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        results = list(\n",
    "            tqdm(\n",
    "                executor.map(process_file, files_to_process),\n",
    "                total=len(files_to_process),\n",
    "                desc=\"Processing files\",\n",
    "                unit=\"file\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Group results by key suffix\n",
    "    for result in results:\n",
    "        if result is None:\n",
    "            continue\n",
    "\n",
    "        key_suffix, timestamp, subdir, df, _ = result\n",
    "\n",
    "        if key_suffix not in grouped_data:\n",
    "            grouped_data[key_suffix] = {\n",
    "                \"files\": [],  # List of file metadata (timestamp, directory, data)\n",
    "                \"group_timestamp\": None,  # Earliest timestamp in the group\n",
    "            }\n",
    "\n",
    "        grouped_data[key_suffix][\"files\"].append({\n",
    "            \"timestamp\": timestamp,\n",
    "            \"directory\": subdir,\n",
    "            \"data\": df,\n",
    "        })\n",
    "\n",
    "    # Calculate group timestamp (earliest timestamp) and sort groups\n",
    "    for key, group in grouped_data.items():\n",
    "        group[\"group_timestamp\"] = min(file[\"timestamp\"] for file in group[\"files\"])\n",
    "\n",
    "    print(\"grouped_data time: \", [gd[\"group_timestamp\"] for gd in [x for x in grouped_data.values()][0:5]])\n",
    "\n",
    "    # Sort the groups by group timestamp\n",
    "    sorted_grouped_data = dict(\n",
    "        sorted(grouped_data.items(), key=lambda item: item[1][\"group_timestamp\"])\n",
    "    )\n",
    "\n",
    "    return sorted_grouped_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e44c7d",
   "metadata": {},
   "source": [
    "## Aggregation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90f0eb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_new_files(groups):\n",
    "    \"\"\"\n",
    "    Process groups into a new_files dictionary, adding group_timestamp to each DataFrame entry\n",
    "    and merging meteora_pairs into enriched_data based on the matching columns.\n",
    "    \"\"\"\n",
    "    new_files = {\"dex_screener_pairs\": [], \"meteora_pairs\": [], \"enriched_data\": []}\n",
    "\n",
    "    for key, group in groups.items():\n",
    "        group_timestamp = group[\"group_timestamp\"]\n",
    "        meteora_pairs = None\n",
    "\n",
    "        # Extract meteora_pairs in advance\n",
    "        for file in group[\"files\"]:\n",
    "            if file[\"directory\"] == \"meteora_pairs\":\n",
    "                meteora_pairs = file[\"data\"].clone().with_columns(pl.lit(group_timestamp).alias(\"group_timestamp\"))\n",
    "                break\n",
    "\n",
    "        for file in group[\"files\"]:\n",
    "            directory = file[\"directory\"]\n",
    "            df = file[\"data\"].clone()\n",
    "\n",
    "            # Add group_timestamp as a new column to the DataFrame\n",
    "            df = df.with_columns(pl.lit(group_timestamp).alias(\"group_timestamp\"))\n",
    "\n",
    "            # If directory is enriched_data, merge meteora_pairs\n",
    "            if directory == \"enriched_data\" and meteora_pairs is not None:\n",
    "                # Select relevant fields from meteora_pairs\n",
    "                meteora_pairs_selected = meteora_pairs.select([\n",
    "                    \"address\",\n",
    "                    \"apr\",\n",
    "                    \"max_fee_percentage\",\n",
    "                    \"cumulative_trade_volume\",\n",
    "                    \"cumulative_fee_volume\",\n",
    "                    \"reserve_x_amount\",\n",
    "                    \"apy\",\n",
    "                    \"base_fee_percentage\",\n",
    "                    \"liquidity\",\n",
    "                    \"hide\",\n",
    "                    \"trade_volume_24h\",\n",
    "                    \"farm_apy\",\n",
    "                    \"reserve_y_amount\",\n",
    "                    \"protocol_fee_percentage\",\n",
    "                    \"today_fees\",\n",
    "                    \"farm_apr\",\n",
    "                    \"fees_24h\"\n",
    "                ])\n",
    "\n",
    "                # Perform the join on \"pairAddress\" and \"address\"\n",
    "                df = df.join(\n",
    "                    meteora_pairs_selected,\n",
    "                    left_on=\"pairAddress\",\n",
    "                    right_on=\"address\",\n",
    "                    how=\"left\"  # Keep all rows from enriched_data\n",
    "                )\n",
    "                \n",
    "            # Append the processed DataFrame to the corresponding directory list\n",
    "            if directory in new_files:\n",
    "                new_files[directory].append(df)\n",
    "\n",
    "    return new_files\n",
    "\n",
    "\n",
    "def aggregate_new_files_with_logging(new_files):\n",
    "    \"\"\"\n",
    "    Aggregate new_files data into one massive Polars DataFrame per directory.\n",
    "    Logs mismatched columns for debugging purposes.\n",
    "    \"\"\"\n",
    "    data_book = {}\n",
    "\n",
    "    for directory, dfs in new_files.items():\n",
    "        if dfs:  # Ensure there are DataFrames to concatenate\n",
    "            # Collect all column names from the DataFrames\n",
    "            all_columns = [set(df.columns) for df in dfs]\n",
    "            common_columns = set.intersection(*all_columns)\n",
    "            mismatched_columns = [columns - common_columns for columns in all_columns]\n",
    "\n",
    "            # Log mismatched columns\n",
    "            # print(f\"\\nDirectory: {directory}\")\n",
    "            for i, mismatch in enumerate(mismatched_columns):\n",
    "                if mismatch:\n",
    "                    print(f\"  DataFrame {i} has additional columns: {mismatch}\")\n",
    "\n",
    "            # Align column names to ensure they match for concatenation\n",
    "            dfs = [df.select(list(common_columns)) for df in dfs]\n",
    "\n",
    "            # Concatenate aligned DataFrames\n",
    "            aggregated_df = pl.concat(dfs)\n",
    "            data_book[f\"aggregated_{directory}\"] = aggregated_df\n",
    "\n",
    "    return data_book\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29415da2",
   "metadata": {},
   "source": [
    "## Data diagnostics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "030f49c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_dataframe_fields(data_book):\n",
    "    \"\"\"\n",
    "    Compare fields between dataframes by identifying columns that exist in dex_screener_pairs and meteora_pairs\n",
    "    but are missing from the enriched dataset. For each missing field, returns the field name, data type,\n",
    "    and source dataframe in a tabulated format.\n",
    "    example:\n",
    "    \n",
    "    usage:\n",
    "        compare_dataframe_fields(data_book)\n",
    "\n",
    "    output:\n",
    "        | Missing Field | Type | Source Dataframe |\n",
    "        | ------------- | ---- | --------------- |\n",
    "        | info.websites[3].url | str | aggregated_dex_screener_pairs |\n",
    "    \"\"\"\n",
    "    # Access the dataframes\n",
    "    aggregated_enriched_data = data_book.get(\"aggregated_enriched_data\")\n",
    "    aggregated_dex_screener_pairs = data_book.get(\"aggregated_dex_screener_pairs\")\n",
    "    aggregated_meteora_pairs = data_book.get(\"aggregated_meteora_pairs\")\n",
    "\n",
    "    # Get columns for each dataframe\n",
    "    enriched_columns = set(aggregated_enriched_data.columns)\n",
    "    dex_screener_columns = set(aggregated_dex_screener_pairs.columns)\n",
    "    meteora_columns = set(aggregated_meteora_pairs.columns)\n",
    "\n",
    "    # Find missing fields\n",
    "    missing_from_enriched_in_dex = dex_screener_columns - enriched_columns\n",
    "    missing_from_enriched_in_meteora = meteora_columns - enriched_columns\n",
    "\n",
    "    # Prepare a list for tabulated output\n",
    "    result = []\n",
    "    \n",
    "    # Add missing fields from dex_screener_pairs\n",
    "    for field in missing_from_enriched_in_dex:\n",
    "        field_type = aggregated_dex_screener_pairs[field].dtype\n",
    "        result.append([field, field_type, \"aggregated_dex_screener_pairs\"])\n",
    "\n",
    "    # Add missing fields from meteora_pairs\n",
    "    for field in missing_from_enriched_in_meteora:\n",
    "        field_type = aggregated_meteora_pairs[field].dtype\n",
    "        result.append([field, field_type, \"aggregated_meteora_pairs\"])\n",
    "\n",
    "    # Print the results in tabulated format\n",
    "    headers = [\"Missing Field\", \"Type\", \"Source Dataframe\"]\n",
    "    print(tabulate(result, headers=headers, tablefmt=\"grid\"))\n",
    "\n",
    "def investigate_schema_mismatches_with_examples(new_files):\n",
    "    \"\"\"\n",
    "    Investigate schema mismatches across DataFrames in each directory of new_files.\n",
    "    Logs mismatched types and provides a summary with example values for each type.\n",
    "    example:\n",
    "        usage:\n",
    "            investigate_schema_mismatches_with_examples(new_files)\n",
    "\n",
    "        output:\n",
    "            Mismatched column types with examples:\n",
    "            Column 'info.websites[3].url' has multiple types:\n",
    "                Type: str, Example: https://www.google.com\n",
    "    \"\"\"\n",
    "    for directory, dfs in new_files.items():\n",
    "        if directory != \"meteora_pairs\":\n",
    "            continue\n",
    "            \n",
    "        if not dfs:\n",
    "            print(f\"\\nDirectory: {directory} has no DataFrames.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nDirectory: {directory}\")\n",
    "        schema_summary = {}\n",
    "\n",
    "        # Collect column types and example values for each DataFrame\n",
    "        for i, df in enumerate(dfs):\n",
    "            # print(f\"  DataFrame {i} schema:\")\n",
    "            schema = df.schema  # Returns a dictionary of column names and types\n",
    "            for col, dtype in schema.items():\n",
    "                # print(f\"    {col}: {dtype}\")\n",
    "\n",
    "                # Track the types seen for each column and example values\n",
    "                if col not in schema_summary:\n",
    "                    schema_summary[col] = {}\n",
    "\n",
    "                if dtype not in schema_summary[col]:\n",
    "                    # Get an example value of the specific type from the column\n",
    "                    try:\n",
    "                        example_value = df.select(col).filter(pl.col(col).is_not_null()).head(1).to_numpy()[0, 0]\n",
    "                    except Exception:\n",
    "                        example_value = None  # Handle cases where getting an example fails\n",
    "                    schema_summary[col][dtype] = example_value\n",
    "\n",
    "        # Identify mismatched types\n",
    "        print(\"\\n  Mismatched column types with examples:\")\n",
    "        for col, types_with_examples in schema_summary.items():\n",
    "            if len(types_with_examples) > 1:  # More than one type found for the column\n",
    "                print(f\"    Column '{col}' has multiple types:\")\n",
    "                for dtype, example in types_with_examples.items():\n",
    "                    print(f\"      Type: {dtype}, Example: {example}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58850a3b",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f8b6b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_side_by_side_tables_from_grouped_data(grouped_data, key, file_index_1, file_index_2, row_index=0):\n",
    "    # Extract the data for the specified rows\n",
    "    data_1 = grouped_data[key][\"files\"][file_index_1][\"data\"][row_index]\n",
    "    data_2 = grouped_data[key][\"files\"][file_index_2][\"data\"][row_index]\n",
    "\n",
    "    return print_side_by_side_tables(data_1, data_2)\n",
    "    \n",
    "def print_side_by_side_tables(raw_data_1, raw_data_2):\n",
    "    \"\"\"\n",
    "    Prints two tables side by side for easy comparison.\n",
    "\n",
    "    This function takes two data inputs, converts them to plain Python types,\n",
    "    and prints them in a tabular format side by side using the `tabulate` library.\n",
    "\n",
    "    Parameters:\n",
    "    - raw_data_1: A data object with a `to_dict()` method that returns a dictionary\n",
    "      where keys are column names and values are data values or objects with a `to_list()` method.\n",
    "    - raw_data_2: A data object similar to `raw_data_1`.\n",
    "\n",
    "    The function assumes that the input data can be converted to a dictionary and that\n",
    "    the values in the dictionary can be converted to lists if they have a `to_list()` method.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    Suppose `raw_data_1` and `raw_data_2` are dataframes with the following content:\n",
    "\n",
    "    raw_data_1:\n",
    "    | Column | Value |\n",
    "    |--------|-------|\n",
    "    | A      | [1, 2]|\n",
    "    | B      | [3, 4]|\n",
    "\n",
    "    raw_data_2:\n",
    "    | Column | Value |\n",
    "    |--------|-------|\n",
    "    | A      | [5, 6]|\n",
    "    | C      | [7, 8]|\n",
    "\n",
    "    The output will be:\n",
    "    +---------+---------+---+---------+---------+\n",
    "    | Column  | Value   | | | Column  | Value   |\n",
    "    +=========+=========+===+=========+=========+\n",
    "    | A       | [1, 2]  | | | A       | [5, 6]  |\n",
    "    +---------+---------+---+---------+---------+\n",
    "    | B       | [3, 4]  | | | C       | [7, 8]  |\n",
    "    +---------+---------+---+---------+---------+\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    data_1 = raw_data_1.to_dict()\n",
    "    data_2 = raw_data_2.to_dict()\n",
    "    \n",
    "    # Convert all values to plain Python types\n",
    "    def convert_to_plain_types(data):\n",
    "        return {k: (v.to_list() if hasattr(v, \"to_list\") else v) for k, v in data.items()}\n",
    "\n",
    "    data_1 = convert_to_plain_types(data_1)\n",
    "    data_2 = convert_to_plain_types(data_2)\n",
    "\n",
    "    # Get the keys (columns) and values for each table\n",
    "    table_1 = [[key, value] for key, value in data_1.items()]\n",
    "    table_2 = [[key, value] for key, value in data_2.items()]\n",
    "\n",
    "    # Ensure both tables have the same number of rows by padding with empty strings\n",
    "    max_rows = max(len(table_1), len(table_2))\n",
    "    while len(table_1) < max_rows:\n",
    "        table_1.append([\"\", \"\"])\n",
    "    while len(table_2) < max_rows:\n",
    "        table_2.append([\"\", \"\"])\n",
    "\n",
    "    # Combine the tables side by side\n",
    "    combined_table = []\n",
    "    for row_1, row_2 in zip(table_1, table_2):\n",
    "        combined_table.append(row_1 + [\"|\"] + row_2)\n",
    "\n",
    "    # Define headers for the tables\n",
    "    headers = [\"Column\", \"Value\", \"|\", \"Column\", \"Value\"]\n",
    "\n",
    "    # Print the combined table using tabulate\n",
    "    print(tabulate(combined_table, headers=headers, tablefmt=\"grid\"))\n",
    "\n",
    "def pretty_print_schema(df):\n",
    "    schema = df.schema\n",
    "    \n",
    "    # Convert schema to a list of tuples for tabulate\n",
    "    schema_list = [(col, dtype) for col, dtype in schema.items()]\n",
    "    \n",
    "    # Pretty-print using tabulate\n",
    "    print(tabulate(schema_list, headers=[\"Column\", \"Data Type\"], tablefmt=\"pretty\"))\n",
    "\n",
    "def visual_analyze(df):\n",
    "    # Filter out rows where the column is None\n",
    "    df_filtered = df.filter(\n",
    "        pl.col(\"time_diff_minutes\").is_not_null()  # Replace \"your_column\" with your actual column name\n",
    "    )\n",
    "    # Convert Polars Series to a list for plotting\n",
    "    time_diff = df_filtered[\"time_diff_minutes\"].to_list()\n",
    "    \n",
    "    # # Create a boxplot\n",
    "    # plt.boxplot(time_diff, vert=False, patch_artist=True, boxprops=dict(facecolor=\"lightblue\"))\n",
    "\n",
    "    # # Create a histogram with density\n",
    "    # sns.histplot(time_diff, kde=True, color=\"skyblue\", bins=10, edgecolor=\"black\")\n",
    "    # plt.title(\"Histogram with Density of Time Differences\")\n",
    "    # plt.ylabel(\"Density\")\n",
    "\n",
    "    # plt.title(\"Boxplot of Time Differences\")\n",
    "\n",
    "    # plt.scatter(range(len(time_diff)), time_diff, color=\"red\", alpha=0.7)\n",
    "    # plt.title(\"Scatter Plot of Time Differences\")\n",
    "    # plt.xlabel(\"Index\")\n",
    "    # plt.ylabel(\"Time Difference\")\n",
    "    # plt.show()\n",
    "    # Example data (replace with your actual data)\n",
    "    # time_diff = [10, 20, 30, 40, 50, 1000, 2000, 10, 20, 30, 40, 50, 1000, 2000, 10, 20, 30, 40, 50]\n",
    "    \n",
    "    # # Create a histogram\n",
    "    # plt.figure(figsize=(8, 6))\n",
    "    # sns.histplot(time_diff, bins=10, color=\"skyblue\", edgecolor=\"black\", kde=False)\n",
    "    # plt.title(\"Histogram of Time Differences\")\n",
    "    # plt.xlabel(\"Time Difference\")\n",
    "    # plt.ylabel(\"Frequency (Number of Values)\")\n",
    "    # plt.show()\n",
    "\n",
    "    # Define ranges and count values in each range\n",
    "    ranges = [\"0-40\", \"40-70\", \"70-1000\",\"1000-2000\", \"2000-3000\", \"3000+\"]\n",
    "    counts = [\n",
    "        sum(0 <= x < 40 for x in time_diff),\n",
    "        sum(40 <= x < 70 for x in time_diff),\n",
    "        sum(70 <= x < 1000 for x in time_diff),\n",
    "        sum(1000 <= x < 2000 for x in time_diff),\n",
    "        sum(2000 <= x < 3000 for x in time_diff),\n",
    "        sum(x >= 3000 for x in time_diff),\n",
    "    ]\n",
    "    \n",
    "    # Create a bar plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(ranges, counts, color=\"orange\", edgecolor=\"black\")\n",
    "    plt.title(\"Bar Plot of Time Differences\")\n",
    "    plt.xlabel(\"Time Difference Range\")\n",
    "    plt.ylabel(\"Frequency (Number of Values)\")\n",
    "    plt.show()\n",
    "\n",
    "# def print_side_by_side_tables(raw_data_1, raw_data_2):\n",
    "#     data_1 = raw_data_1.to_dict()\n",
    "#     data_2 = raw_data_2.to_dict()\n",
    "    \n",
    "#     # Convert all values to plain Python types\n",
    "#     def convert_to_plain_types(data):\n",
    "#         return {k: (v.to_list() if hasattr(v, \"to_list\") else v) for k, v in data.items()}\n",
    "\n",
    "#     data_1 = convert_to_plain_types(data_1)\n",
    "#     data_2 = convert_to_plain_types(data_2)\n",
    "\n",
    "#     # Get the keys (columns) and values for each table\n",
    "#     table_1 = [[key, value] for key, value in data_1.items()]\n",
    "#     table_2 = [[key, value] for key, value in data_2.items()]\n",
    "\n",
    "#     # Ensure both tables have the same number of rows by padding with empty strings\n",
    "#     max_rows = max(len(table_1), len(table_2))\n",
    "#     while len(table_1) < max_rows:\n",
    "#         table_1.append([\"\", \"\"])\n",
    "#     while len(table_2) < max_rows:\n",
    "#         table_2.append([\"\", \"\"])\n",
    "\n",
    "#     # Combine the tables side by side\n",
    "#     combined_table = []\n",
    "#     for row_1, row_2 in zip(table_1, table_2):\n",
    "#         combined_table.append(row_1 + [\"|\"] + row_2)\n",
    "\n",
    "#     # Define headers for the tables\n",
    "#     headers = [\"Column\", \"Value\", \"|\", \"Column\", \"Value\"]\n",
    "\n",
    "#     # Print the combined table using tabulate\n",
    "#     print(tabulate(combined_table, headers=headers, tablefmt=\"grid\"))\n",
    "\n",
    "def pretty_print_schema(df):\n",
    "    schema = df.schema\n",
    "    \n",
    "    # Convert schema to a list of tuples for tabulate\n",
    "    schema_list = [(col, dtype) for col, dtype in schema.items()]\n",
    "    \n",
    "    # Pretty-print using tabulate\n",
    "    print(tabulate(schema_list, headers=[\"Column\", \"Data Type\"], tablefmt=\"pretty\"))\n",
    "\n",
    "def visual_analyze(df):\n",
    "    # Filter out rows where the column is None\n",
    "    df_filtered = df.filter(\n",
    "        pl.col(\"time_diff_minutes\").is_not_null()  # Replace \"your_column\" with your actual column name\n",
    "    )\n",
    "    # Convert Polars Series to a list for plotting\n",
    "    time_diff = df_filtered[\"time_diff_minutes\"].to_list()\n",
    "    \n",
    "    # # Create a boxplot\n",
    "    # plt.boxplot(time_diff, vert=False, patch_artist=True, boxprops=dict(facecolor=\"lightblue\"))\n",
    "\n",
    "    # # Create a histogram with density\n",
    "    # sns.histplot(time_diff, kde=True, color=\"skyblue\", bins=10, edgecolor=\"black\")\n",
    "    # plt.title(\"Histogram with Density of Time Differences\")\n",
    "    # plt.ylabel(\"Density\")\n",
    "\n",
    "    # plt.title(\"Boxplot of Time Differences\")\n",
    "\n",
    "    # plt.scatter(range(len(time_diff)), time_diff, color=\"red\", alpha=0.7)\n",
    "    # plt.title(\"Scatter Plot of Time Differences\")\n",
    "    # plt.xlabel(\"Index\")\n",
    "    # plt.ylabel(\"Time Difference\")\n",
    "    # plt.show()\n",
    "    # Example data (replace with your actual data)\n",
    "    # time_diff = [10, 20, 30, 40, 50, 1000, 2000, 10, 20, 30, 40, 50, 1000, 2000, 10, 20, 30, 40, 50]\n",
    "    \n",
    "    # # Create a histogram\n",
    "    # plt.figure(figsize=(8, 6))\n",
    "    # sns.histplot(time_diff, bins=10, color=\"skyblue\", edgecolor=\"black\", kde=False)\n",
    "    # plt.title(\"Histogram of Time Differences\")\n",
    "    # plt.xlabel(\"Time Difference\")\n",
    "    # plt.ylabel(\"Frequency (Number of Values)\")\n",
    "    # plt.show()\n",
    "\n",
    "    # Define ranges and count values in each range\n",
    "    ranges = [\"0-40\", \"40-70\", \"70-1000\",\"1000-2000\", \"2000-3000\", \"3000+\"]\n",
    "    counts = [\n",
    "        sum(0 <= x < 40 for x in time_diff),\n",
    "        sum(40 <= x < 70 for x in time_diff),\n",
    "        sum(70 <= x < 1000 for x in time_diff),\n",
    "        sum(1000 <= x < 2000 for x in time_diff),\n",
    "        sum(2000 <= x < 3000 for x in time_diff),\n",
    "        sum(x >= 3000 for x in time_diff),\n",
    "    ]\n",
    "    \n",
    "    # Create a bar plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(ranges, counts, color=\"orange\", edgecolor=\"black\")\n",
    "    plt.title(\"Bar Plot of Time Differences\")\n",
    "    plt.xlabel(\"Time Difference Range\")\n",
    "    plt.ylabel(\"Frequency (Number of Values)\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba205a2f",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b492a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_analyze(df):\n",
    "    \"\"\"\n",
    "    Statistical analysis of the time difference between group_timestamp and group_timestamp_future.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate mean and standard deviation\n",
    "    mean = df[\"time_diff_minutes\"].mean()\n",
    "    std = df[\"time_diff_minutes\"].std()\n",
    "    \n",
    "    # Compute Z-Score\n",
    "    df = df.with_columns(\n",
    "        ((pl.col(\"time_diff_minutes\") - mean) / std).alias(\"z_score\")\n",
    "    )\n",
    "    \n",
    "    # Filter outliers (|Z| > 3)\n",
    "    outliers = df.filter(\n",
    "        pl.col(\"z_score\").abs() > 3\n",
    "    )\n",
    "\n",
    "    return df\n",
    "    \n",
    "def compute_profit_columns(df):\n",
    "    # 1) Compute pairAge and isPumpToken\n",
    "    df = df.with_columns([\n",
    "        (pl.col(\"group_timestamp\") - pl.col(\"pairCreatedAt\")).alias(\"pairAge\"),\n",
    "        pl.col(\"baseToken.address\").str.ends_with(\"pump\").alias(\"isPumpToken\"),\n",
    "    ])\n",
    "\n",
    "    # 2) Create a 'future' DataFrame for self-join\n",
    "    df_future = df.select([\n",
    "        \"pairAddress\",\n",
    "        pl.col(\"group_timestamp\").alias(\"group_timestamp_future\"),\n",
    "        pl.col(\"liquidity.meteora\").alias(\"liquidity_future\"),\n",
    "        pl.col(\"cumulative_fee_volume\").alias(\"cfv_future\"),\n",
    "    ])\n",
    "    \n",
    "\n",
    "    # 3) Sort for asof join\n",
    "    df = df.sort([\"pairAddress\", \"group_timestamp\"])\n",
    "    df_future = df_future.sort([\"pairAddress\", \"group_timestamp_future\"])\n",
    "\n",
    "    # FIX HERE: Remove [] from left_on/right_on\n",
    "    joined = df.join_asof(\n",
    "        df_future,\n",
    "        left_on=\"group_timestamp\",  # <- string, not list\n",
    "        right_on=\"group_timestamp_future\",  # <- string, not list\n",
    "        by=\"pairAddress\",\n",
    "        strategy=\"forward\",\n",
    "        allow_exact_matches=False\n",
    "    )\n",
    "\n",
    "    # Calculate time difference in minutes between timestamps\n",
    "    joined = joined.with_columns(\n",
    "        ((pl.col(\"group_timestamp_future\") - pl.col(\"group_timestamp\")) / 1000).alias(\"time_diff_seconds\"),\n",
    "        ((pl.col(\"group_timestamp_future\") - pl.col(\"group_timestamp\")) / 1000 /60).alias(\"time_diff_minutes\")\n",
    "    )\n",
    "\n",
    "    # 4) Filter time window\n",
    "    joined = joined.filter(\n",
    "        (pl.col(\"time_diff_minutes\") >= 40) & (pl.col(\"time_diff_minutes\") < 70)\n",
    "    )\n",
    "\n",
    "\n",
    "    # Convert cumulative_fee_volume to float before calculations\n",
    "    joined = joined.with_columns([\n",
    "        pl.col(\"cumulative_fee_volume\").cast(pl.Float64).alias(\"cumulative_fee_volume\"),\n",
    "        pl.col(\"cfv_future\").cast(pl.Float64).alias(\"cfv_future\"),\n",
    "    ])\n",
    "\n",
    "    # First create averageLiquidity\n",
    "    joined = joined.with_columns(\n",
    "        ((pl.col(\"liquidity.meteora\") + pl.col(\"liquidity_future\")) / 2).alias(\"averageLiquidity\")\n",
    "    )\n",
    "    \n",
    "    # Then calculate percentageProfit using the newly created column\n",
    "    joined = joined.filter(\n",
    "        pl.col(\"averageLiquidity\") > 0\n",
    "    )\n",
    "    joined = joined.with_columns(\n",
    "        ((pl.col(\"cfv_future\") - pl.col(\"cumulative_fee_volume\")) / pl.col(\"averageLiquidity\") * 100).alias(\"percentageProfit\")\n",
    "    )\n",
    "\n",
    "    return joined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9318645",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "550f1957",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LSTM Model for Liquidity Pool Profitability Prediction\n",
    "Steps: Data Loading → Preprocessing → Feature Engineering → Sequence Creation → Model Training → Evaluation\n",
    "\"\"\"\n",
    "\n",
    "class LiquidityPoolPredictor:\n",
    "    \"\"\"\n",
    "    Static class for LSTM-based liquidity pool profitability prediction\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def preprocess_data(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Applies critical preprocessing steps specific to liquidity pool data\n",
    "        Justification: Prepares raw data for temporal modeling while preserving key financial relationships\n",
    "        \"\"\"\n",
    "        # Feature selection (based on domain knowledge)\n",
    "        keep_cols = [\n",
    "            'volume.m5', 'volume.h1', 'volume.h6', 'volume.h24',\n",
    "            'liquidity.meteora', 'liquidity.usd', 'apr', 'farm_apy',\n",
    "            'priceChange.h1', 'priceChange.m5', 'priceChange.h24',\n",
    "            'txns.m5.sells', 'txns.h1.buys', 'percentageProfit',\n",
    "            'group_timestamp', 'pairCreatedAt'\n",
    "        ]\n",
    "        # keep_cols = [\n",
    "        #     'volume.m5', 'volume.h1', 'volume.h6', 'volume.h24',\n",
    "        #     'liquidity.meteora', 'liquidity.usd', 'apr', 'apy', 'farm_apy',\n",
    "        #     'priceChange.h1', 'priceChange.m5', 'priceChange.h24',\n",
    "        #     'txns.m5.sells', 'txns.h1.buys', 'time_diff_seconds',\n",
    "        #     'time_diff_minutes', 'group_timestamp', 'percentageProfit',\n",
    "        #     'pairCreatedAt'\n",
    "        # ]\n",
    "        df = df.select(keep_cols)\n",
    "        \n",
    "        # Handle missing data\n",
    "        numerical_cols = [col for col in df.columns if df[col].dtype in [pl.Float64, pl.Int64]]\n",
    "        df = df.with_columns(\n",
    "            [pl.col(col).fill_null(pl.col(col).median()) for col in numerical_cols]\n",
    "        )\n",
    "        \n",
    "        # Temporal alignment\n",
    "        df = df.sort('group_timestamp')\n",
    "        \n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def engineer_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates time-aware features critical for LSTM performance\n",
    "        Justification: Captures liquidity pool dynamics through lagged effects and rolling metrics\n",
    "        \"\"\"\n",
    "        # Lag features (critical for autoregressive patterns)\n",
    "        lag_config = {\n",
    "            'volume.m5': [1, 3, 6],\n",
    "            'priceChange.h1': [12, 24],\n",
    "            'txns.m5.sells': [1, 2]\n",
    "        }\n",
    "        \n",
    "        for col, lags in lag_config.items():\n",
    "            for lag in lags:\n",
    "                df = df.with_columns(\n",
    "                    pl.col(col).shift(lag).alias(f\"{col}_lag{lag}\")\n",
    "                )\n",
    "        \n",
    "        # Rolling statistics (captures market regime changes)\n",
    "        window_sizes = {'liquidity.usd': 12, 'apr': 24}\n",
    "        for col, window in window_sizes.items():\n",
    "            df = df.with_columns(\n",
    "                pl.col(col).rolling_mean(window).alias(f\"{col}_rolling_mean_{window}\"),\n",
    "                pl.col(col).rolling_std(window).alias(f\"{col}_rolling_std_{window}\")\n",
    "            )\n",
    "        \n",
    "        # Time-based features (accounts for temporal patterns)\n",
    "        df = df.with_columns(\n",
    "            (pl.col('group_timestamp') - pl.col('pairCreatedAt')).alias('pool_age'),\n",
    "            pl.col('group_timestamp').cast(pl.Datetime).dt.hour().alias('hour_of_day'),\n",
    "            pl.col('group_timestamp').cast(pl.Datetime).dt.weekday().alias('day_of_week')\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_data(df: pl.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Applies robust scaling to handle crypto market outliers\n",
    "        Justification: Preserves temporal relationships while constraining values for LSTM stability\n",
    "        \"\"\"\n",
    "        # Separate target variable\n",
    "        target = df['percentageProfit'].to_numpy()\n",
    "        features = df.drop('percentageProfit').to_numpy()\n",
    "        \n",
    "        # Robust scaling for volatile market data\n",
    "        scaler = RobustScaler()\n",
    "        scaled_features = scaler.fit_transform(features)\n",
    "        \n",
    "        # # MinMax scaling for percentage-based features\n",
    "        # pct_cols = [df.columns.index(col) for col in []]\n",
    "        # # pct_cols = [df.columns.index(col) for col in ['apr', 'apy', 'farm_apy']]\n",
    "        # minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        # scaled_features[:, pct_cols] = minmax_scaler.fit_transform(scaled_features[:, pct_cols])\n",
    "        \n",
    "        return scaled_features, target\n",
    "\n",
    "    @staticmethod\n",
    "    def create_sequences(features: np.ndarray, target: np.ndarray, seq_length: int = 12) -> tuple:\n",
    "        \"\"\"\n",
    "        Transforms tabular data into LSTM-ready sequences\n",
    "        Justification: Captures temporal dependencies critical for liquidity pool dynamics\n",
    "        \"\"\"\n",
    "        X, y = [], []\n",
    "        for i in range(len(features) - seq_length):\n",
    "            X.append(features[i:i+seq_length])\n",
    "            y.append(target[i+seq_length])\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_lstm_model(input_shape: tuple) -> Sequential:\n",
    "        \"\"\"\n",
    "        Constructs optimized LSTM architecture for financial time series\n",
    "        Justification: Balances model capacity with regularization needs for volatile crypto data\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            LSTM(64, \n",
    "                 input_shape=input_shape,\n",
    "                 return_sequences=True,\n",
    "                 dropout=0.2,\n",
    "                 recurrent_dropout=0.1,\n",
    "                 kernel_regularizer=l2(0.01)),\n",
    "            LSTM(32,\n",
    "                 dropout=0.2,\n",
    "                 recurrent_dropout=0.1,\n",
    "                 kernel_regularizer=l2(0.01)),\n",
    "            Dense(16, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        optimizer = AdamW(learning_rate=0.001, weight_decay=0.005)\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss=Huber(delta=0.5),\n",
    "                      metrics=['mae', 'mse'])\n",
    "        \n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_model(model: Sequential, X_test: np.ndarray, y_test: np.ndarray):\n",
    "        \"\"\"\n",
    "        Comprehensive model evaluation with financial metrics\n",
    "        Justification: Provides actionable insights beyond standard ML metrics\n",
    "        \"\"\"\n",
    "        # Standard metrics\n",
    "        test_loss, test_mae, test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "        \n",
    "        # Prediction visualization\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(y_test, label='Actual Returns')\n",
    "        plt.plot(predictions, label='Predicted Returns', alpha=0.7)\n",
    "        plt.title('Liquidity Pool Return Predictions vs Actual')\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Percentage Profit')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        # Trading strategy simulation\n",
    "        strategy_returns = np.sign(predictions.flatten()) * y_test\n",
    "        cumulative_returns = np.cumprod(1 + strategy_returns)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(cumulative_returns, label='Strategy Returns')\n",
    "        plt.plot(np.cumprod(1 + y_test), label='Buy & Hold')\n",
    "        plt.title('Cumulative Returns Comparison')\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Cumulative Return')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        return test_mae\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5849bf20",
   "metadata": {},
   "source": [
    "## .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08233d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a variable to track the previous timestamp\n",
    "previous_timestamp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fce50ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Store the current timestamp\n",
    "current_timestamp = time.time()\n",
    "\n",
    "# Format the current timestamp to a user-friendly format\n",
    "formatted_current_time = datetime.fromtimestamp(current_timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Print the formatted current timestamp\n",
    "print(\"Current Timestamp:\", formatted_current_time)\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "# Print the difference between the current and previous timestamp if previous exists\n",
    "if previous_timestamp is not None:\n",
    "    time_difference = current_timestamp - previous_timestamp\n",
    "    formatted_difference = str(timedelta(seconds=time_difference))\n",
    "    print(\"Previous Timestamp:\", datetime.fromtimestamp(previous_timestamp).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    print(\"Difference:\", formatted_difference)\n",
    "\n",
    "# Track the previous timestamp\n",
    "previous_timestamp = current_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f4e60d",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d15fb",
   "metadata": {},
   "source": [
    "## Load data from FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a902f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory containing the subdirectories\n",
    "base_directory = \"/home/dev/mined-data\"\n",
    "# Load and group data by key suffix\n",
    "start_time = time.time()\n",
    "grouped_data = load_and_group_json_data_by_suffix_parallel(base_directory, max_files=1)\n",
    "end_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1ff175",
   "metadata": {},
   "source": [
    "## Aggregate data across files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fe423bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_files = process_new_files(grouped_data) \n",
    "data_book = aggregate_new_files_with_logging(new_files) \n",
    "\n",
    "for key, df in data_book.items():\n",
    "    print(f\"{key}: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a88723d",
   "metadata": {},
   "source": [
    "## Filter enriched data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bef8c91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregated_enriched_data = data_book.get(\"aggregated_enriched_data\")\n",
    "aggregated_enriched_data = data_book.get(\"aggregated_enriched_data\")\n",
    "SOL_TOKEN = \"So11111111111111111111111111111111111111112\"\n",
    "aggregated_enriched_data = aggregated_enriched_data.filter(pl.col(\"quoteToken.address\") == SOL_TOKEN)\n",
    "# print(len(aggregat/ed_enriched_data))\n",
    "aggregated_enriched_data = aggregated_enriched_data.filter(pl.col(\"volume.h6\") > 0)  \n",
    "print(len(aggregated_enriched_data))\n",
    "\n",
    "# joined.sort([\"percentageProfit\"], descending=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e1f306",
   "metadata": {},
   "source": [
    "# Action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ad32506",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_enriched_data = compute_profit_columns(aggregated_enriched_data)\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Process data through pipeline\n",
    "df = aggregated_enriched_data\n",
    "processed_df = LiquidityPoolPredictor.preprocess_data(df)\n",
    "engineered_df = LiquidityPoolPredictor.engineer_features(processed_df)\n",
    "scaled_features, target = LiquidityPoolPredictor.normalize_data(engineered_df)\n",
    "X, y = LiquidityPoolPredictor.create_sequences(scaled_features, target)\n",
    "\n",
    "# Train/test split\n",
    "train_size = int(0.7 * len(X))\n",
    "val_size = int(0.2 * len(X))\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "# # Build and train model\n",
    "# model = LiquidityPoolPredictor.build_lstm_model((X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "#     monitor='val_loss',\n",
    "#     patience=10,\n",
    "#     restore_best_weights=True\n",
    "# )\n",
    "\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     epochs=100,\n",
    "#     batch_size=64,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     callbacks=[early_stopping],\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # Evaluate model\n",
    "# test_mae = LiquidityPoolPredictor.evaluate_model(model, X_test, y_test)\n",
    "# print(f\"Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "# # Save model\n",
    "# model.save(\"lstm_liquidity_pool_predictor.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014d4b4c",
   "metadata": {},
   "source": [
    "## Scratchpad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e4399d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_file(enriched_file, enriched_data_dir, meteora_pairs_dir, output_dir, meteora_files, processed_files):\n",
    "    if enriched_file in processed_files:\n",
    "        return\n",
    "\n",
    "    enriched_file_path = os.path.join(enriched_data_dir, enriched_file)\n",
    "    with open(enriched_file_path, 'r') as ef:\n",
    "        enriched_data = json.load(ef)\n",
    "\n",
    "    key = enriched_file.split('_')[-1].split('.')[0]\n",
    "    meteora_file = next((f for f in meteora_files if key in f), None)\n",
    "    \n",
    "    if meteora_file:\n",
    "        meteora_file_path = os.path.join(meteora_pairs_dir, meteora_file)\n",
    "        with open(meteora_file_path, 'r') as mf:\n",
    "            meteora_data = json.load(mf)\n",
    "\n",
    "        meteora_data_dict = {m['address']: m for m in meteora_data}\n",
    "        for dex_screener_pair in enriched_data:\n",
    "            meteora_pair = meteora_data_dict.get(dex_screener_pair['pairAddress'])\n",
    "            if meteora_pair:\n",
    "                dex_screener_pair['meteora'] = meteora_pair\n",
    "\n",
    "        output_file_path = os.path.join(output_dir, enriched_file)\n",
    "        with open(output_file_path, 'w') as ef:\n",
    "            json.dump(enriched_data, ef, indent=2)\n",
    "\n",
    "def align_files(enriched_data_dir, meteora_pairs_dir, output_dir):\n",
    "    enriched_files = os.listdir(enriched_data_dir)\n",
    "    meteora_files = os.listdir(meteora_pairs_dir)\n",
    "    processed_files = set(os.listdir(output_dir))\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        list(tqdm(executor.map(lambda enriched_file: process_file(enriched_file, enriched_data_dir, meteora_pairs_dir, output_dir, meteora_files, processed_files), enriched_files), total=len(enriched_files), desc=\"Processing files\"))\n",
    "\n",
    "align_files('/home/dev/mined-data/enriched_data', '/home/dev/mined-data/meteora_pairs', '/home/dev/mined-data/enriched_dex_pairs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8e4dd28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dokku ps:scale meteora-data-miner web=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60565cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f98a861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f08a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tf version: \", tf.__version__)\n",
    "print(\"keras version: \", tf.keras.__version__)\n",
    "# print(\"scipy version: \", scipy.__version__)\n",
    "print(\"numpy version: \", np.__version__)\n",
    "print(\"polars version: \", pl.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bed3e8-d48d-4e6d-9e25-80f1132a3948",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3829c3-1632-4b6d-abb7-6139ca6495c1",
   "metadata": {},
   "source": [
    "- Big contribution to misalignment on dex_screener_pairs data is:\n",
    "        boosts\n",
    "            .active\n",
    "        \n",
    "        info\n",
    "            .websites[]\n",
    "              .label\n",
    "              .url\n",
    "  Will consider expanding on them in the future for additional features. Will remove for now.\n",
    "- uv add scipy==1.12 (numpy.char module not found error)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
